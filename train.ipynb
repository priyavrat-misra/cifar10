{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4377f614db5f471c91e85fa08416e9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9afc7def4428437da23b283971b5bf11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6ab219bdadd40fb8e209456de089ec0",
              "IPY_MODEL_9772b06f4e12463489f552aa166aaf69"
            ]
          }
        },
        "9afc7def4428437da23b283971b5bf11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6ab219bdadd40fb8e209456de089ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e178acac648481794c294d3cd7b19b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8dbd1a03e64a4a85babf359a54e9f59e"
          }
        },
        "9772b06f4e12463489f552aa166aaf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49942ec4e6544270a4b112dd25ebe3a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 98073130.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e01910f8844e4c899b73f8028f78cc4a"
          }
        },
        "0e178acac648481794c294d3cd7b19b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8dbd1a03e64a4a85babf359a54e9f59e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49942ec4e6544270a4b112dd25ebe3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e01910f8844e4c899b73f8028f78cc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfIDt-ZZsIHm"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import device, get_num_correct, RunBuilder\n",
        "from network import Network"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGmTRbzEsIHr",
        "outputId": "358f38bf-be13-4d64-f0cc-e4c1cb663dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97,
          "referenced_widgets": [
            "4377f614db5f471c91e85fa08416e9d8",
            "9afc7def4428437da23b283971b5bf11",
            "c6ab219bdadd40fb8e209456de089ec0",
            "9772b06f4e12463489f552aa166aaf69",
            "0e178acac648481794c294d3cd7b19b0",
            "8dbd1a03e64a4a85babf359a54e9f59e",
            "49942ec4e6544270a4b112dd25ebe3a9",
            "e01910f8844e4c899b73f8028f78cc4a"
          ]
        }
      },
      "source": [
        "# covertes to tensor and normalizes the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# load the test set\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, num_workers=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4377f614db5f471c91e85fa08416e9d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op58QlVssIHv"
      },
      "source": [
        "Before starting the training process, it is often a best practice to try and overfit a single batch of data, so to confirm that the network is implemented correctly and it has the capability to be used as the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsrwComRsIHv",
        "outputId": "db22db8f-9eb8-4bbd-d7e7-13d423da0979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load the train_set for trying out the model\n",
        "loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=1)\n",
        "\n",
        "model = Network()  # initialize the NN\n",
        "criterion = nn.CrossEntropyLoss()  # loss function (categorical cross-entropy)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # specify the optimizer\n",
        "images, labels = next(iter(loader))  # load one batch of train_set\n",
        "\n",
        "for epoch in range(50):\n",
        "    correct = 0  # will be used to track the running num correct\n",
        "    preds = model(images)  # forward pass\n",
        "    loss = criterion(preds, labels)  # calculate loss\n",
        "    optimizer.zero_grad()  # clear accumulated gradients from the previous pass\n",
        "    loss.backward()  # backward pass\n",
        "    optimizer.step()  # perform a single optimization step\n",
        "    correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "    # print statistics\n",
        "    print(f'epoch: {epoch+1:2d}\\tloss:{loss.item():2.4f}\\tacc:{(correct/32):2.4f}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1\tloss:2.3374\tacc:0.0625\n",
            "epoch:  2\tloss:7.6343\tacc:0.1875\n",
            "epoch:  3\tloss:6.3814\tacc:0.1562\n",
            "epoch:  4\tloss:4.1736\tacc:0.1250\n",
            "epoch:  5\tloss:2.8999\tacc:0.1875\n",
            "epoch:  6\tloss:2.3920\tacc:0.1875\n",
            "epoch:  7\tloss:2.1777\tacc:0.1875\n",
            "epoch:  8\tloss:2.0633\tacc:0.3750\n",
            "epoch:  9\tloss:1.9401\tacc:0.3125\n",
            "epoch: 10\tloss:1.8869\tacc:0.3438\n",
            "epoch: 11\tloss:1.9343\tacc:0.3438\n",
            "epoch: 12\tloss:1.7509\tacc:0.4062\n",
            "epoch: 13\tloss:1.7444\tacc:0.3125\n",
            "epoch: 14\tloss:1.6313\tacc:0.4688\n",
            "epoch: 15\tloss:1.6949\tacc:0.3438\n",
            "epoch: 16\tloss:1.5455\tacc:0.4062\n",
            "epoch: 17\tloss:1.4715\tacc:0.4375\n",
            "epoch: 18\tloss:1.4672\tacc:0.4688\n",
            "epoch: 19\tloss:1.2785\tacc:0.5312\n",
            "epoch: 20\tloss:1.1871\tacc:0.5938\n",
            "epoch: 21\tloss:1.2549\tacc:0.4062\n",
            "epoch: 22\tloss:1.1221\tacc:0.5312\n",
            "epoch: 23\tloss:0.8853\tacc:0.6875\n",
            "epoch: 24\tloss:0.8454\tacc:0.6875\n",
            "epoch: 25\tloss:0.7457\tacc:0.7812\n",
            "epoch: 26\tloss:0.6501\tacc:0.7500\n",
            "epoch: 27\tloss:0.7226\tacc:0.6875\n",
            "epoch: 28\tloss:0.7279\tacc:0.6875\n",
            "epoch: 29\tloss:0.5334\tacc:0.8438\n",
            "epoch: 30\tloss:0.4670\tacc:0.8438\n",
            "epoch: 31\tloss:0.4966\tacc:0.8125\n",
            "epoch: 32\tloss:0.5061\tacc:0.8750\n",
            "epoch: 33\tloss:0.7123\tacc:0.7188\n",
            "epoch: 34\tloss:0.2277\tacc:0.9375\n",
            "epoch: 35\tloss:0.5369\tacc:0.8125\n",
            "epoch: 36\tloss:0.4720\tacc:0.7500\n",
            "epoch: 37\tloss:0.2040\tacc:0.9688\n",
            "epoch: 38\tloss:0.4443\tacc:0.9375\n",
            "epoch: 39\tloss:0.3460\tacc:0.9062\n",
            "epoch: 40\tloss:0.2414\tacc:0.9375\n",
            "epoch: 41\tloss:0.1668\tacc:0.9688\n",
            "epoch: 42\tloss:0.1970\tacc:0.9375\n",
            "epoch: 43\tloss:0.2950\tacc:0.8750\n",
            "epoch: 44\tloss:0.1184\tacc:1.0000\n",
            "epoch: 45\tloss:0.2233\tacc:0.9375\n",
            "epoch: 46\tloss:0.1043\tacc:1.0000\n",
            "epoch: 47\tloss:0.2585\tacc:0.8750\n",
            "epoch: 48\tloss:0.2586\tacc:0.9062\n",
            "epoch: 49\tloss:0.0673\tacc:1.0000\n",
            "epoch: 50\tloss:0.1135\tacc:1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdnqudBDsIHz"
      },
      "source": [
        "As we can see, the model is overfitting which means the network implementation is correct! Now we can continue with our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoWPNbbVsIH0"
      },
      "source": [
        "valid_size = 0.2  # percentage of train_set to use it as validation\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_set)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj3VnZhusIH3"
      },
      "source": [
        "# for hyper-parameter search\n",
        "from collections import OrderedDict\n",
        "\n",
        "params = OrderedDict(\n",
        "    lr = [0.01, 0.003, 0.001],\n",
        "    batch_size = [64, 128, 256, 512]\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUq5gbZRsIH6",
        "outputId": "c68d886d-6c10-4ded-86a3-15579b0323c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()  # loss function (categorical cross-entropy)\n",
        "\n",
        "# iterate through the cross product of hyper-parameters defined in params\n",
        "for run in RunBuilder.get_runs(params):\n",
        "    print(f'\\n{run}')\n",
        "    model = Network().to(device)  # initialize the NN\n",
        "    # load the train set\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=run.batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=1\n",
        "    )\n",
        "    # load the validation set\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=run.batch_size,\n",
        "        sampler=valid_sampler,\n",
        "        num_workers=1\n",
        "    )\n",
        "    optimizer = optim.Adam(model.parameters(), lr=run.lr)  # specify the optimizer\n",
        "\n",
        "    comment = f'-{run}'  # will be used for naming the runs based on each run's hyper-parameters\n",
        "    tb = SummaryWriter(comment=comment)\n",
        "\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf  # set initial minimum to infinity\n",
        "\n",
        "    num_epochs = 30  # number of epochs used for training\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_correct = 0, 0  # wil be used to track the running loss and correct\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()  # set the model to train mode\n",
        "        for batch in train_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device (cpu/gpu)\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate loss\n",
        "            optimizer.zero_grad()  # clear accumulated gradients from the previous pass\n",
        "            loss.backward()  # backward pass\n",
        "            optimizer.step()  # perform a single optimization step\n",
        "\n",
        "            train_loss += loss.item() * run.batch_size  # update the running loss\n",
        "            train_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        tb.add_scalar('Train Loss', train_loss, epoch)  # add train_loss for the current epoch to tensorboard\n",
        "        tb.add_scalar('Train Accuracy', train_correct/len(train_loader.sampler), epoch)\n",
        "\n",
        "        model.eval()  # set the model to evaluation mode\n",
        "        with torch.no_grad():  # turn off grad tracking, as we don't need gradients for validation\n",
        "            valid_loss, valid_correct = 0, 0  # will be used to track the running validation loss and correct\n",
        "            ######################\n",
        "            # validate the model #\n",
        "            ######################\n",
        "            for batch in valid_loader:\n",
        "                images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "                preds = model(images)  # forward pass\n",
        "                loss = criterion(preds, labels)  # calculate the loss  \n",
        "\n",
        "                valid_loss += loss.item() * run.batch_size  # update the running loss\n",
        "                valid_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "            tb.add_scalar('Validation Loss', valid_loss, epoch)  # add valid_loss for the current epoch to tensorboard\n",
        "            tb.add_scalar('Validation Accuracy', valid_correct/len(valid_loader.sampler), epoch)\n",
        "\n",
        "            # print training/validation statistics\n",
        "            # calculate average loss over an epoch\n",
        "            train_loss = train_loss/len(train_loader.sampler)\n",
        "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "            print(f'Epoch {epoch+1:2d}: Training Loss: {train_loss:.6f} Validation Loss: {valid_loss:.6f}')\n",
        "\n",
        "            # save model if validation loss has decreased\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                print(f'\\t  valid_loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f})  saving model...')\n",
        "                torch.save(model.state_dict(), f'./models/model-{run}.ckpt')\n",
        "                valid_loss_min = valid_loss\n",
        "\n",
        "            # load the model with least validation loss i.e., load the one which was saved most recently\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    f'./models/model-{run}.ckpt',\n",
        "                    map_location=device\n",
        "                )\n",
        "            )\n",
        "\n",
        "            test_loss, test_correct = 0, 0  # will be used to track the running test loss and correct\n",
        "            ##################\n",
        "            # test the model #\n",
        "            ##################\n",
        "            for batch in test_loader:\n",
        "                images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "                preds = model(images)  # forward pass\n",
        "                loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "                test_loss += loss.item() * 64  # update the running loss\n",
        "                test_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "            tb.add_scalar('Test Loss', test_loss, epoch)\n",
        "            tb.add_scalar('Test Accuracy', test_correct/len(test_set), epoch)\n",
        "\n",
        "        # iterate the parameters' weights and it's grads and plot their historgrams to tensorboard\n",
        "        # (will be helpful for checking if the model is having the vanishing gradient problem)\n",
        "        for name, weight in model.named_parameters():\n",
        "            tb.add_histogram(name, weight, epoch)\n",
        "            tb.add_histogram(f'{name}.grad', weight.grad, epoch)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "run(lr=0.01, batch_size=64)\n",
            "Epoch  1: Training Loss: 1.767838 Validation Loss: 1.437015\n",
            "\t  valid_loss decreased (inf --> 1.437015)  saving model...\n",
            "Epoch  2: Training Loss: 1.430668 Validation Loss: 1.304457\n",
            "\t  valid_loss decreased (1.437015 --> 1.304457)  saving model...\n",
            "Epoch  3: Training Loss: 1.287644 Validation Loss: 1.165301\n",
            "\t  valid_loss decreased (1.304457 --> 1.165301)  saving model...\n",
            "Epoch  4: Training Loss: 1.204075 Validation Loss: 1.064403\n",
            "\t  valid_loss decreased (1.165301 --> 1.064403)  saving model...\n",
            "Epoch  5: Training Loss: 1.136221 Validation Loss: 1.072901\n",
            "Epoch  6: Training Loss: 1.138143 Validation Loss: 1.030662\n",
            "\t  valid_loss decreased (1.064403 --> 1.030662)  saving model...\n",
            "Epoch  7: Training Loss: 1.091750 Validation Loss: 1.009098\n",
            "\t  valid_loss decreased (1.030662 --> 1.009098)  saving model...\n",
            "Epoch  8: Training Loss: 1.045550 Validation Loss: 0.990777\n",
            "\t  valid_loss decreased (1.009098 --> 0.990777)  saving model...\n",
            "Epoch  9: Training Loss: 1.016373 Validation Loss: 1.006573\n",
            "Epoch 10: Training Loss: 1.016200 Validation Loss: 0.954627\n",
            "\t  valid_loss decreased (0.990777 --> 0.954627)  saving model...\n",
            "Epoch 11: Training Loss: 0.980973 Validation Loss: 0.994269\n",
            "Epoch 12: Training Loss: 0.979504 Validation Loss: 0.930732\n",
            "\t  valid_loss decreased (0.954627 --> 0.930732)  saving model...\n",
            "Epoch 13: Training Loss: 0.962804 Validation Loss: 0.963730\n",
            "Epoch 14: Training Loss: 0.959507 Validation Loss: 0.942805\n",
            "Epoch 15: Training Loss: 0.953312 Validation Loss: 0.960085\n",
            "Epoch 16: Training Loss: 0.954290 Validation Loss: 0.921400\n",
            "\t  valid_loss decreased (0.930732 --> 0.921400)  saving model...\n",
            "Epoch 17: Training Loss: 0.935799 Validation Loss: 0.929667\n",
            "Epoch 18: Training Loss: 0.931258 Validation Loss: 0.894013\n",
            "\t  valid_loss decreased (0.921400 --> 0.894013)  saving model...\n",
            "Epoch 19: Training Loss: 0.928775 Validation Loss: 0.920487\n",
            "Epoch 20: Training Loss: 0.922044 Validation Loss: 0.899921\n",
            "Epoch 21: Training Loss: 0.913512 Validation Loss: 0.913739\n",
            "Epoch 22: Training Loss: 0.914126 Validation Loss: 0.879540\n",
            "\t  valid_loss decreased (0.894013 --> 0.879540)  saving model...\n",
            "Epoch 23: Training Loss: 0.905903 Validation Loss: 0.889034\n",
            "Epoch 24: Training Loss: 0.895210 Validation Loss: 0.921295\n",
            "Epoch 25: Training Loss: 0.900618 Validation Loss: 0.944733\n",
            "Epoch 26: Training Loss: 0.898189 Validation Loss: 0.920634\n",
            "Epoch 27: Training Loss: 0.894413 Validation Loss: 0.920914\n",
            "Epoch 28: Training Loss: 0.895750 Validation Loss: 0.890340\n",
            "Epoch 29: Training Loss: 0.896239 Validation Loss: 0.892022\n",
            "Epoch 30: Training Loss: 0.889899 Validation Loss: 0.903776\n",
            "\n",
            "run(lr=0.01, batch_size=128)\n",
            "Epoch  1: Training Loss: 1.780134 Validation Loss: 1.545652\n",
            "\t  valid_loss decreased (inf --> 1.545652)  saving model...\n",
            "Epoch  2: Training Loss: 1.425102 Validation Loss: 1.247004\n",
            "\t  valid_loss decreased (1.545652 --> 1.247004)  saving model...\n",
            "Epoch  3: Training Loss: 1.262730 Validation Loss: 1.143262\n",
            "\t  valid_loss decreased (1.247004 --> 1.143262)  saving model...\n",
            "Epoch  4: Training Loss: 1.147892 Validation Loss: 1.043543\n",
            "\t  valid_loss decreased (1.143262 --> 1.043543)  saving model...\n",
            "Epoch  5: Training Loss: 1.077718 Validation Loss: 0.974463\n",
            "\t  valid_loss decreased (1.043543 --> 0.974463)  saving model...\n",
            "Epoch  6: Training Loss: 1.019749 Validation Loss: 0.936165\n",
            "\t  valid_loss decreased (0.974463 --> 0.936165)  saving model...\n",
            "Epoch  7: Training Loss: 0.969196 Validation Loss: 0.931926\n",
            "\t  valid_loss decreased (0.936165 --> 0.931926)  saving model...\n",
            "Epoch  8: Training Loss: 0.943746 Validation Loss: 0.905436\n",
            "\t  valid_loss decreased (0.931926 --> 0.905436)  saving model...\n",
            "Epoch  9: Training Loss: 0.909984 Validation Loss: 0.934088\n",
            "Epoch 10: Training Loss: 0.913238 Validation Loss: 0.883769\n",
            "\t  valid_loss decreased (0.905436 --> 0.883769)  saving model...\n",
            "Epoch 11: Training Loss: 0.888788 Validation Loss: 0.873231\n",
            "\t  valid_loss decreased (0.883769 --> 0.873231)  saving model...\n",
            "Epoch 12: Training Loss: 0.869735 Validation Loss: 0.886047\n",
            "Epoch 13: Training Loss: 0.861645 Validation Loss: 0.853509\n",
            "\t  valid_loss decreased (0.873231 --> 0.853509)  saving model...\n",
            "Epoch 14: Training Loss: 0.852679 Validation Loss: 0.844277\n",
            "\t  valid_loss decreased (0.853509 --> 0.844277)  saving model...\n",
            "Epoch 15: Training Loss: 0.829295 Validation Loss: 0.857652\n",
            "Epoch 16: Training Loss: 0.824148 Validation Loss: 0.863353\n",
            "Epoch 17: Training Loss: 0.833667 Validation Loss: 0.866649\n",
            "Epoch 18: Training Loss: 0.832291 Validation Loss: 0.862459\n",
            "Epoch 19: Training Loss: 0.823764 Validation Loss: 0.884786\n",
            "Epoch 20: Training Loss: 0.838506 Validation Loss: 0.894984\n",
            "Epoch 21: Training Loss: 0.829003 Validation Loss: 0.926967\n",
            "Epoch 22: Training Loss: 0.823687 Validation Loss: 0.878408\n",
            "Epoch 23: Training Loss: 0.826443 Validation Loss: 0.902808\n",
            "Epoch 24: Training Loss: 0.829695 Validation Loss: 0.853075\n",
            "Epoch 25: Training Loss: 0.825226 Validation Loss: 0.858097\n",
            "Epoch 26: Training Loss: 0.831518 Validation Loss: 0.839821\n",
            "\t  valid_loss decreased (0.844277 --> 0.839821)  saving model...\n",
            "Epoch 27: Training Loss: 0.812642 Validation Loss: 0.888530\n",
            "Epoch 28: Training Loss: 0.817659 Validation Loss: 0.868496\n",
            "Epoch 29: Training Loss: 0.813472 Validation Loss: 0.834623\n",
            "\t  valid_loss decreased (0.839821 --> 0.834623)  saving model...\n",
            "Epoch 30: Training Loss: 0.803203 Validation Loss: 0.824257\n",
            "\t  valid_loss decreased (0.834623 --> 0.824257)  saving model...\n",
            "\n",
            "run(lr=0.01, batch_size=256)\n",
            "Epoch  1: Training Loss: 1.945911 Validation Loss: 1.624338\n",
            "\t  valid_loss decreased (inf --> 1.624338)  saving model...\n",
            "Epoch  2: Training Loss: 1.529354 Validation Loss: 1.367000\n",
            "\t  valid_loss decreased (1.624338 --> 1.367000)  saving model...\n",
            "Epoch  3: Training Loss: 1.360466 Validation Loss: 1.357222\n",
            "\t  valid_loss decreased (1.367000 --> 1.357222)  saving model...\n",
            "Epoch  4: Training Loss: 1.239193 Validation Loss: 1.114751\n",
            "\t  valid_loss decreased (1.357222 --> 1.114751)  saving model...\n",
            "Epoch  5: Training Loss: 1.141943 Validation Loss: 1.083758\n",
            "\t  valid_loss decreased (1.114751 --> 1.083758)  saving model...\n",
            "Epoch  6: Training Loss: 1.058832 Validation Loss: 1.002542\n",
            "\t  valid_loss decreased (1.083758 --> 1.002542)  saving model...\n",
            "Epoch  7: Training Loss: 1.005984 Validation Loss: 0.963132\n",
            "\t  valid_loss decreased (1.002542 --> 0.963132)  saving model...\n",
            "Epoch  8: Training Loss: 0.957814 Validation Loss: 0.938645\n",
            "\t  valid_loss decreased (0.963132 --> 0.938645)  saving model...\n",
            "Epoch  9: Training Loss: 0.921758 Validation Loss: 0.959349\n",
            "Epoch 10: Training Loss: 0.929205 Validation Loss: 0.904869\n",
            "\t  valid_loss decreased (0.938645 --> 0.904869)  saving model...\n",
            "Epoch 11: Training Loss: 0.886114 Validation Loss: 0.896112\n",
            "\t  valid_loss decreased (0.904869 --> 0.896112)  saving model...\n",
            "Epoch 12: Training Loss: 0.861494 Validation Loss: 0.870402\n",
            "\t  valid_loss decreased (0.896112 --> 0.870402)  saving model...\n",
            "Epoch 13: Training Loss: 0.843746 Validation Loss: 0.891285\n",
            "Epoch 14: Training Loss: 0.840712 Validation Loss: 0.859544\n",
            "\t  valid_loss decreased (0.870402 --> 0.859544)  saving model...\n",
            "Epoch 15: Training Loss: 0.810528 Validation Loss: 0.888367\n",
            "Epoch 16: Training Loss: 0.821832 Validation Loss: 0.826496\n",
            "\t  valid_loss decreased (0.859544 --> 0.826496)  saving model...\n",
            "Epoch 17: Training Loss: 0.787068 Validation Loss: 0.847313\n",
            "Epoch 18: Training Loss: 0.783988 Validation Loss: 0.810631\n",
            "\t  valid_loss decreased (0.826496 --> 0.810631)  saving model...\n",
            "Epoch 19: Training Loss: 0.771420 Validation Loss: 0.869366\n",
            "Epoch 20: Training Loss: 0.775694 Validation Loss: 0.864444\n",
            "Epoch 21: Training Loss: 0.766239 Validation Loss: 0.863862\n",
            "Epoch 22: Training Loss: 0.763895 Validation Loss: 0.825496\n",
            "Epoch 23: Training Loss: 0.763056 Validation Loss: 0.861897\n",
            "Epoch 24: Training Loss: 0.767148 Validation Loss: 0.830321\n",
            "Epoch 25: Training Loss: 0.768913 Validation Loss: 0.883797\n",
            "Epoch 26: Training Loss: 0.768013 Validation Loss: 0.843303\n",
            "Epoch 27: Training Loss: 0.777789 Validation Loss: 0.870196\n",
            "Epoch 28: Training Loss: 0.769079 Validation Loss: 0.823076\n",
            "Epoch 29: Training Loss: 0.777546 Validation Loss: 0.822181\n",
            "Epoch 30: Training Loss: 0.768550 Validation Loss: 0.813067\n",
            "\n",
            "run(lr=0.01, batch_size=512)\n",
            "Epoch  1: Training Loss: 2.087766 Validation Loss: 1.667065\n",
            "\t  valid_loss decreased (inf --> 1.667065)  saving model...\n",
            "Epoch  2: Training Loss: 1.591228 Validation Loss: 1.463746\n",
            "\t  valid_loss decreased (1.667065 --> 1.463746)  saving model...\n",
            "Epoch  3: Training Loss: 1.446872 Validation Loss: 1.364341\n",
            "\t  valid_loss decreased (1.463746 --> 1.364341)  saving model...\n",
            "Epoch  4: Training Loss: 1.326706 Validation Loss: 1.337538\n",
            "\t  valid_loss decreased (1.364341 --> 1.337538)  saving model...\n",
            "Epoch  5: Training Loss: 1.231656 Validation Loss: 1.271916\n",
            "\t  valid_loss decreased (1.337538 --> 1.271916)  saving model...\n",
            "Epoch  6: Training Loss: 1.142189 Validation Loss: 1.045950\n",
            "\t  valid_loss decreased (1.271916 --> 1.045950)  saving model...\n",
            "Epoch  7: Training Loss: 1.067207 Validation Loss: 0.998636\n",
            "\t  valid_loss decreased (1.045950 --> 0.998636)  saving model...\n",
            "Epoch  8: Training Loss: 1.010667 Validation Loss: 0.955302\n",
            "\t  valid_loss decreased (0.998636 --> 0.955302)  saving model...\n",
            "Epoch  9: Training Loss: 0.966532 Validation Loss: 0.938340\n",
            "\t  valid_loss decreased (0.955302 --> 0.938340)  saving model...\n",
            "Epoch 10: Training Loss: 0.913510 Validation Loss: 0.940984\n",
            "Epoch 11: Training Loss: 0.920301 Validation Loss: 0.933286\n",
            "\t  valid_loss decreased (0.938340 --> 0.933286)  saving model...\n",
            "Epoch 12: Training Loss: 0.905744 Validation Loss: 0.852517\n",
            "\t  valid_loss decreased (0.933286 --> 0.852517)  saving model...\n",
            "Epoch 13: Training Loss: 0.868850 Validation Loss: 0.873420\n",
            "Epoch 14: Training Loss: 0.858932 Validation Loss: 0.882359\n",
            "Epoch 15: Training Loss: 0.855367 Validation Loss: 0.859747\n",
            "Epoch 16: Training Loss: 0.860389 Validation Loss: 0.918822\n",
            "Epoch 17: Training Loss: 0.859729 Validation Loss: 0.890637\n",
            "Epoch 18: Training Loss: 0.862888 Validation Loss: 0.887723\n",
            "Epoch 19: Training Loss: 0.868230 Validation Loss: 0.839044\n",
            "\t  valid_loss decreased (0.852517 --> 0.839044)  saving model...\n",
            "Epoch 20: Training Loss: 0.831727 Validation Loss: 0.843516\n",
            "Epoch 21: Training Loss: 0.830985 Validation Loss: 1.008875\n",
            "Epoch 22: Training Loss: 0.838696 Validation Loss: 0.835178\n",
            "\t  valid_loss decreased (0.839044 --> 0.835178)  saving model...\n",
            "Epoch 23: Training Loss: 0.814721 Validation Loss: 0.863426\n",
            "Epoch 24: Training Loss: 0.808170 Validation Loss: 0.842004\n",
            "Epoch 25: Training Loss: 0.828593 Validation Loss: 0.832226\n",
            "\t  valid_loss decreased (0.835178 --> 0.832226)  saving model...\n",
            "Epoch 26: Training Loss: 0.800480 Validation Loss: 0.825301\n",
            "\t  valid_loss decreased (0.832226 --> 0.825301)  saving model...\n",
            "Epoch 27: Training Loss: 0.780382 Validation Loss: 0.824492\n",
            "\t  valid_loss decreased (0.825301 --> 0.824492)  saving model...\n",
            "Epoch 28: Training Loss: 0.761565 Validation Loss: 0.829465\n",
            "Epoch 29: Training Loss: 0.755436 Validation Loss: 0.818433\n",
            "\t  valid_loss decreased (0.824492 --> 0.818433)  saving model...\n",
            "Epoch 30: Training Loss: 0.741738 Validation Loss: 0.842170\n",
            "\n",
            "run(lr=0.003, batch_size=64)\n",
            "Epoch  1: Training Loss: 1.533662 Validation Loss: 1.292525\n",
            "\t  valid_loss decreased (inf --> 1.292525)  saving model...\n",
            "Epoch  2: Training Loss: 1.230487 Validation Loss: 1.143544\n",
            "\t  valid_loss decreased (1.292525 --> 1.143544)  saving model...\n",
            "Epoch  3: Training Loss: 1.068530 Validation Loss: 0.961985\n",
            "\t  valid_loss decreased (1.143544 --> 0.961985)  saving model...\n",
            "Epoch  4: Training Loss: 0.960561 Validation Loss: 0.933975\n",
            "\t  valid_loss decreased (0.961985 --> 0.933975)  saving model...\n",
            "Epoch  5: Training Loss: 0.893731 Validation Loss: 0.871210\n",
            "\t  valid_loss decreased (0.933975 --> 0.871210)  saving model...\n",
            "Epoch  6: Training Loss: 0.838355 Validation Loss: 0.891370\n",
            "Epoch  7: Training Loss: 0.837632 Validation Loss: 0.816259\n",
            "\t  valid_loss decreased (0.871210 --> 0.816259)  saving model...\n",
            "Epoch  8: Training Loss: 0.781199 Validation Loss: 0.788394\n",
            "\t  valid_loss decreased (0.816259 --> 0.788394)  saving model...\n",
            "Epoch  9: Training Loss: 0.747238 Validation Loss: 0.766100\n",
            "\t  valid_loss decreased (0.788394 --> 0.766100)  saving model...\n",
            "Epoch 10: Training Loss: 0.707898 Validation Loss: 0.779163\n",
            "Epoch 11: Training Loss: 0.715423 Validation Loss: 0.803797\n",
            "Epoch 12: Training Loss: 0.717132 Validation Loss: 0.761288\n",
            "\t  valid_loss decreased (0.766100 --> 0.761288)  saving model...\n",
            "Epoch 13: Training Loss: 0.680546 Validation Loss: 0.777234\n",
            "Epoch 14: Training Loss: 0.675955 Validation Loss: 0.752827\n",
            "\t  valid_loss decreased (0.761288 --> 0.752827)  saving model...\n",
            "Epoch 15: Training Loss: 0.656463 Validation Loss: 0.743506\n",
            "\t  valid_loss decreased (0.752827 --> 0.743506)  saving model...\n",
            "Epoch 16: Training Loss: 0.628162 Validation Loss: 0.754687\n",
            "Epoch 17: Training Loss: 0.625812 Validation Loss: 0.722979\n",
            "\t  valid_loss decreased (0.743506 --> 0.722979)  saving model...\n",
            "Epoch 18: Training Loss: 0.607237 Validation Loss: 0.788825\n",
            "Epoch 19: Training Loss: 0.606064 Validation Loss: 0.737192\n",
            "Epoch 20: Training Loss: 0.604289 Validation Loss: 0.736417\n",
            "Epoch 21: Training Loss: 0.607651 Validation Loss: 0.741882\n",
            "Epoch 22: Training Loss: 0.605053 Validation Loss: 0.765075\n",
            "Epoch 23: Training Loss: 0.609469 Validation Loss: 0.760939\n",
            "Epoch 24: Training Loss: 0.607256 Validation Loss: 0.733860\n",
            "Epoch 25: Training Loss: 0.599833 Validation Loss: 0.745616\n",
            "Epoch 26: Training Loss: 0.601469 Validation Loss: 0.749977\n",
            "Epoch 27: Training Loss: 0.607461 Validation Loss: 0.762958\n",
            "Epoch 28: Training Loss: 0.607433 Validation Loss: 0.739497\n",
            "Epoch 29: Training Loss: 0.602307 Validation Loss: 0.810308\n",
            "Epoch 30: Training Loss: 0.604834 Validation Loss: 0.768600\n",
            "\n",
            "run(lr=0.003, batch_size=128)\n",
            "Epoch  1: Training Loss: 1.546725 Validation Loss: 1.273804\n",
            "\t  valid_loss decreased (inf --> 1.273804)  saving model...\n",
            "Epoch  2: Training Loss: 1.209081 Validation Loss: 1.049827\n",
            "\t  valid_loss decreased (1.273804 --> 1.049827)  saving model...\n",
            "Epoch  3: Training Loss: 1.027346 Validation Loss: 0.963676\n",
            "\t  valid_loss decreased (1.049827 --> 0.963676)  saving model...\n",
            "Epoch  4: Training Loss: 0.928227 Validation Loss: 0.860649\n",
            "\t  valid_loss decreased (0.963676 --> 0.860649)  saving model...\n",
            "Epoch  5: Training Loss: 0.847156 Validation Loss: 0.800033\n",
            "\t  valid_loss decreased (0.860649 --> 0.800033)  saving model...\n",
            "Epoch  6: Training Loss: 0.791890 Validation Loss: 0.754395\n",
            "\t  valid_loss decreased (0.800033 --> 0.754395)  saving model...\n",
            "Epoch  7: Training Loss: 0.733679 Validation Loss: 0.746838\n",
            "\t  valid_loss decreased (0.754395 --> 0.746838)  saving model...\n",
            "Epoch  8: Training Loss: 0.697235 Validation Loss: 0.732267\n",
            "\t  valid_loss decreased (0.746838 --> 0.732267)  saving model...\n",
            "Epoch  9: Training Loss: 0.660500 Validation Loss: 0.722621\n",
            "\t  valid_loss decreased (0.732267 --> 0.722621)  saving model...\n",
            "Epoch 10: Training Loss: 0.632163 Validation Loss: 0.724263\n",
            "Epoch 11: Training Loss: 0.626537 Validation Loss: 0.719877\n",
            "\t  valid_loss decreased (0.722621 --> 0.719877)  saving model...\n",
            "Epoch 12: Training Loss: 0.594431 Validation Loss: 0.735223\n",
            "Epoch 13: Training Loss: 0.593344 Validation Loss: 0.749178\n",
            "Epoch 14: Training Loss: 0.594257 Validation Loss: 0.712315\n",
            "\t  valid_loss decreased (0.719877 --> 0.712315)  saving model...\n",
            "Epoch 15: Training Loss: 0.564504 Validation Loss: 0.688622\n",
            "\t  valid_loss decreased (0.712315 --> 0.688622)  saving model...\n",
            "Epoch 16: Training Loss: 0.543994 Validation Loss: 0.689578\n",
            "Epoch 17: Training Loss: 0.544716 Validation Loss: 0.728903\n",
            "Epoch 18: Training Loss: 0.549710 Validation Loss: 0.750058\n",
            "Epoch 19: Training Loss: 0.548522 Validation Loss: 0.698360\n",
            "Epoch 20: Training Loss: 0.540930 Validation Loss: 0.707292\n",
            "Epoch 21: Training Loss: 0.546596 Validation Loss: 0.693449\n",
            "Epoch 22: Training Loss: 0.547670 Validation Loss: 0.713403\n",
            "Epoch 23: Training Loss: 0.541047 Validation Loss: 0.698561\n",
            "Epoch 24: Training Loss: 0.547639 Validation Loss: 0.724259\n",
            "Epoch 25: Training Loss: 0.550782 Validation Loss: 0.707981\n",
            "Epoch 26: Training Loss: 0.541946 Validation Loss: 0.675770\n",
            "\t  valid_loss decreased (0.688622 --> 0.675770)  saving model...\n",
            "Epoch 27: Training Loss: 0.521494 Validation Loss: 0.697292\n",
            "Epoch 28: Training Loss: 0.537708 Validation Loss: 0.692616\n",
            "Epoch 29: Training Loss: 0.525146 Validation Loss: 0.739465\n",
            "Epoch 30: Training Loss: 0.521105 Validation Loss: 0.698087\n",
            "\n",
            "run(lr=0.003, batch_size=256)\n",
            "Epoch  1: Training Loss: 1.577232 Validation Loss: 1.307619\n",
            "\t  valid_loss decreased (inf --> 1.307619)  saving model...\n",
            "Epoch  2: Training Loss: 1.222852 Validation Loss: 1.101828\n",
            "\t  valid_loss decreased (1.307619 --> 1.101828)  saving model...\n",
            "Epoch  3: Training Loss: 1.066619 Validation Loss: 1.061706\n",
            "\t  valid_loss decreased (1.101828 --> 1.061706)  saving model...\n",
            "Epoch  4: Training Loss: 0.960791 Validation Loss: 0.965849\n",
            "\t  valid_loss decreased (1.061706 --> 0.965849)  saving model...\n",
            "Epoch  5: Training Loss: 0.887409 Validation Loss: 0.873252\n",
            "\t  valid_loss decreased (0.965849 --> 0.873252)  saving model...\n",
            "Epoch  6: Training Loss: 0.817340 Validation Loss: 0.812888\n",
            "\t  valid_loss decreased (0.873252 --> 0.812888)  saving model...\n",
            "Epoch  7: Training Loss: 0.776337 Validation Loss: 0.812445\n",
            "\t  valid_loss decreased (0.812888 --> 0.812445)  saving model...\n",
            "Epoch  8: Training Loss: 0.720461 Validation Loss: 0.778605\n",
            "\t  valid_loss decreased (0.812445 --> 0.778605)  saving model...\n",
            "Epoch  9: Training Loss: 0.686453 Validation Loss: 0.758035\n",
            "\t  valid_loss decreased (0.778605 --> 0.758035)  saving model...\n",
            "Epoch 10: Training Loss: 0.656327 Validation Loss: 0.749981\n",
            "\t  valid_loss decreased (0.758035 --> 0.749981)  saving model...\n",
            "Epoch 11: Training Loss: 0.616848 Validation Loss: 0.750124\n",
            "Epoch 12: Training Loss: 0.618015 Validation Loss: 0.747120\n",
            "\t  valid_loss decreased (0.749981 --> 0.747120)  saving model...\n",
            "Epoch 13: Training Loss: 0.592813 Validation Loss: 0.728466\n",
            "\t  valid_loss decreased (0.747120 --> 0.728466)  saving model...\n",
            "Epoch 14: Training Loss: 0.560197 Validation Loss: 0.748446\n",
            "Epoch 15: Training Loss: 0.561699 Validation Loss: 0.746886\n",
            "Epoch 16: Training Loss: 0.563848 Validation Loss: 0.736645\n",
            "Epoch 17: Training Loss: 0.565513 Validation Loss: 0.751913\n",
            "Epoch 18: Training Loss: 0.566463 Validation Loss: 0.748918\n",
            "Epoch 19: Training Loss: 0.560893 Validation Loss: 0.755359\n",
            "Epoch 20: Training Loss: 0.563363 Validation Loss: 0.780458\n",
            "Epoch 21: Training Loss: 0.569394 Validation Loss: 0.728253\n",
            "\t  valid_loss decreased (0.728466 --> 0.728253)  saving model...\n",
            "Epoch 22: Training Loss: 0.531556 Validation Loss: 0.712562\n",
            "\t  valid_loss decreased (0.728253 --> 0.712562)  saving model...\n",
            "Epoch 23: Training Loss: 0.521537 Validation Loss: 0.751932\n",
            "Epoch 24: Training Loss: 0.530786 Validation Loss: 0.787266\n",
            "Epoch 25: Training Loss: 0.513590 Validation Loss: 0.736235\n",
            "Epoch 26: Training Loss: 0.525730 Validation Loss: 0.735006\n",
            "Epoch 27: Training Loss: 0.524872 Validation Loss: 0.710396\n",
            "\t  valid_loss decreased (0.712562 --> 0.710396)  saving model...\n",
            "Epoch 28: Training Loss: 0.501430 Validation Loss: 0.725829\n",
            "Epoch 29: Training Loss: 0.496004 Validation Loss: 0.722566\n",
            "Epoch 30: Training Loss: 0.496606 Validation Loss: 0.731487\n",
            "\n",
            "run(lr=0.003, batch_size=512)\n",
            "Epoch  1: Training Loss: 1.621653 Validation Loss: 1.395082\n",
            "\t  valid_loss decreased (inf --> 1.395082)  saving model...\n",
            "Epoch  2: Training Loss: 1.286560 Validation Loss: 1.256213\n",
            "\t  valid_loss decreased (1.395082 --> 1.256213)  saving model...\n",
            "Epoch  3: Training Loss: 1.119371 Validation Loss: 1.052508\n",
            "\t  valid_loss decreased (1.256213 --> 1.052508)  saving model...\n",
            "Epoch  4: Training Loss: 1.017994 Validation Loss: 1.017916\n",
            "\t  valid_loss decreased (1.052508 --> 1.017916)  saving model...\n",
            "Epoch  5: Training Loss: 0.927983 Validation Loss: 0.915659\n",
            "\t  valid_loss decreased (1.017916 --> 0.915659)  saving model...\n",
            "Epoch  6: Training Loss: 0.870140 Validation Loss: 1.240023\n",
            "Epoch  7: Training Loss: 0.878695 Validation Loss: 0.880508\n",
            "\t  valid_loss decreased (0.915659 --> 0.880508)  saving model...\n",
            "Epoch  8: Training Loss: 0.816682 Validation Loss: 0.859543\n",
            "\t  valid_loss decreased (0.880508 --> 0.859543)  saving model...\n",
            "Epoch  9: Training Loss: 0.779529 Validation Loss: 0.778138\n",
            "\t  valid_loss decreased (0.859543 --> 0.778138)  saving model...\n",
            "Epoch 10: Training Loss: 0.719455 Validation Loss: 0.839905\n",
            "Epoch 11: Training Loss: 0.729375 Validation Loss: 0.910399\n",
            "Epoch 12: Training Loss: 0.738731 Validation Loss: 0.827518\n",
            "Epoch 13: Training Loss: 0.723735 Validation Loss: 0.785764\n",
            "Epoch 14: Training Loss: 0.731093 Validation Loss: 0.801719\n",
            "Epoch 15: Training Loss: 0.732070 Validation Loss: 0.811408\n",
            "Epoch 16: Training Loss: 0.717237 Validation Loss: 0.809518\n",
            "Epoch 17: Training Loss: 0.736928 Validation Loss: 0.813197\n",
            "Epoch 18: Training Loss: 0.724540 Validation Loss: 0.772127\n",
            "\t  valid_loss decreased (0.778138 --> 0.772127)  saving model...\n",
            "Epoch 19: Training Loss: 0.689938 Validation Loss: 0.812401\n",
            "Epoch 20: Training Loss: 0.683664 Validation Loss: 0.801228\n",
            "Epoch 21: Training Loss: 0.691592 Validation Loss: 0.777259\n",
            "Epoch 22: Training Loss: 0.692074 Validation Loss: 0.769853\n",
            "\t  valid_loss decreased (0.772127 --> 0.769853)  saving model...\n",
            "Epoch 23: Training Loss: 0.659882 Validation Loss: 0.833290\n",
            "Epoch 24: Training Loss: 0.666858 Validation Loss: 0.741632\n",
            "\t  valid_loss decreased (0.769853 --> 0.741632)  saving model...\n",
            "Epoch 25: Training Loss: 0.619314 Validation Loss: 0.764447\n",
            "Epoch 26: Training Loss: 0.626499 Validation Loss: 0.822036\n",
            "Epoch 27: Training Loss: 0.637435 Validation Loss: 0.751813\n",
            "Epoch 28: Training Loss: 0.620956 Validation Loss: 0.758412\n",
            "Epoch 29: Training Loss: 0.624487 Validation Loss: 0.747154\n",
            "Epoch 30: Training Loss: 0.622557 Validation Loss: 0.777735\n",
            "\n",
            "run(lr=0.001, batch_size=64)\n",
            "Epoch  1: Training Loss: 1.440200 Validation Loss: 1.189012\n",
            "\t  valid_loss decreased (inf --> 1.189012)  saving model...\n",
            "Epoch  2: Training Loss: 1.110637 Validation Loss: 0.973317\n",
            "\t  valid_loss decreased (1.189012 --> 0.973317)  saving model...\n",
            "Epoch  3: Training Loss: 0.965681 Validation Loss: 0.919872\n",
            "\t  valid_loss decreased (0.973317 --> 0.919872)  saving model...\n",
            "Epoch  4: Training Loss: 0.884222 Validation Loss: 0.864602\n",
            "\t  valid_loss decreased (0.919872 --> 0.864602)  saving model...\n",
            "Epoch  5: Training Loss: 0.820679 Validation Loss: 0.802667\n",
            "\t  valid_loss decreased (0.864602 --> 0.802667)  saving model...\n",
            "Epoch  6: Training Loss: 0.759856 Validation Loss: 0.749292\n",
            "\t  valid_loss decreased (0.802667 --> 0.749292)  saving model...\n",
            "Epoch  7: Training Loss: 0.712081 Validation Loss: 0.753048\n",
            "Epoch  8: Training Loss: 0.713951 Validation Loss: 0.781041\n",
            "Epoch  9: Training Loss: 0.709693 Validation Loss: 0.758270\n",
            "Epoch 10: Training Loss: 0.713810 Validation Loss: 0.716144\n",
            "\t  valid_loss decreased (0.749292 --> 0.716144)  saving model...\n",
            "Epoch 11: Training Loss: 0.667136 Validation Loss: 0.751276\n",
            "Epoch 12: Training Loss: 0.669874 Validation Loss: 0.698746\n",
            "\t  valid_loss decreased (0.716144 --> 0.698746)  saving model...\n",
            "Epoch 13: Training Loss: 0.633484 Validation Loss: 0.689181\n",
            "\t  valid_loss decreased (0.698746 --> 0.689181)  saving model...\n",
            "Epoch 14: Training Loss: 0.594980 Validation Loss: 0.689624\n",
            "Epoch 15: Training Loss: 0.593846 Validation Loss: 0.721557\n",
            "Epoch 17: Training Loss: 0.598637 Validation Loss: 0.715952\n",
            "Epoch 18: Training Loss: 0.595431 Validation Loss: 0.704518\n",
            "Epoch 19: Training Loss: 0.597480 Validation Loss: 0.713017\n",
            "Epoch 20: Training Loss: 0.597811 Validation Loss: 0.737922\n",
            "Epoch 21: Training Loss: 0.594916 Validation Loss: 0.690613\n",
            "Epoch 22: Training Loss: 0.599956 Validation Loss: 0.701748\n",
            "Epoch 23: Training Loss: 0.592410 Validation Loss: 0.683177\n",
            "\t  valid_loss decreased (0.689181 --> 0.683177)  saving model...\n",
            "Epoch 24: Training Loss: 0.561319 Validation Loss: 0.739446\n",
            "Epoch 25: Training Loss: 0.567754 Validation Loss: 0.739572\n",
            "Epoch 26: Training Loss: 0.564257 Validation Loss: 0.698297\n",
            "Epoch 27: Training Loss: 0.560300 Validation Loss: 0.761014\n",
            "Epoch 28: Training Loss: 0.559914 Validation Loss: 0.689318\n",
            "Epoch 29: Training Loss: 0.564486 Validation Loss: 0.687680\n",
            "Epoch 30: Training Loss: 0.557515 Validation Loss: 0.744336\n",
            "\n",
            "run(lr=0.001, batch_size=128)\n",
            "Epoch  1: Training Loss: 1.459342 Validation Loss: 1.188419\n",
            "\t  valid_loss decreased (inf --> 1.188419)  saving model...\n",
            "Epoch  2: Training Loss: 1.118585 Validation Loss: 0.995769\n",
            "\t  valid_loss decreased (1.188419 --> 0.995769)  saving model...\n",
            "Epoch  3: Training Loss: 0.986441 Validation Loss: 0.939551\n",
            "\t  valid_loss decreased (0.995769 --> 0.939551)  saving model...\n",
            "Epoch  4: Training Loss: 0.896978 Validation Loss: 0.889851\n",
            "\t  valid_loss decreased (0.939551 --> 0.889851)  saving model...\n",
            "Epoch  5: Training Loss: 0.841424 Validation Loss: 0.861008\n",
            "\t  valid_loss decreased (0.889851 --> 0.861008)  saving model...\n",
            "Epoch  6: Training Loss: 0.783970 Validation Loss: 0.831719\n",
            "\t  valid_loss decreased (0.861008 --> 0.831719)  saving model...\n",
            "Epoch  7: Training Loss: 0.744355 Validation Loss: 0.783347\n",
            "\t  valid_loss decreased (0.831719 --> 0.783347)  saving model...\n",
            "Epoch  8: Training Loss: 0.696812 Validation Loss: 0.750402\n",
            "\t  valid_loss decreased (0.783347 --> 0.750402)  saving model...\n",
            "Epoch  9: Training Loss: 0.659609 Validation Loss: 0.736099\n",
            "\t  valid_loss decreased (0.750402 --> 0.736099)  saving model...\n",
            "Epoch 10: Training Loss: 0.624833 Validation Loss: 0.727571\n",
            "\t  valid_loss decreased (0.736099 --> 0.727571)  saving model...\n",
            "Epoch 11: Training Loss: 0.587838 Validation Loss: 0.742795\n",
            "Epoch 12: Training Loss: 0.588507 Validation Loss: 0.725742\n",
            "\t  valid_loss decreased (0.727571 --> 0.725742)  saving model...\n",
            "Epoch 13: Training Loss: 0.562023 Validation Loss: 0.713050\n",
            "\t  valid_loss decreased (0.725742 --> 0.713050)  saving model...\n",
            "Epoch 14: Training Loss: 0.534393 Validation Loss: 0.715694\n",
            "Epoch 15: Training Loss: 0.531600 Validation Loss: 0.760001\n",
            "Epoch 16: Training Loss: 0.533811 Validation Loss: 0.714569\n",
            "Epoch 17: Training Loss: 0.531024 Validation Loss: 0.716874\n",
            "Epoch 18: Training Loss: 0.531096 Validation Loss: 0.721252\n",
            "Epoch 19: Training Loss: 0.532366 Validation Loss: 0.757429\n",
            "Epoch 20: Training Loss: 0.532685 Validation Loss: 0.739767\n",
            "Epoch 21: Training Loss: 0.535932 Validation Loss: 0.703600\n",
            "\t  valid_loss decreased (0.713050 --> 0.703600)  saving model...\n",
            "Epoch 22: Training Loss: 0.508230 Validation Loss: 0.710712\n",
            "Epoch 23: Training Loss: 0.502078 Validation Loss: 0.690600\n",
            "\t  valid_loss decreased (0.703600 --> 0.690600)  saving model...\n",
            "Epoch 24: Training Loss: 0.482811 Validation Loss: 0.714313\n",
            "Epoch 25: Training Loss: 0.485352 Validation Loss: 0.693168\n",
            "Epoch 26: Training Loss: 0.488693 Validation Loss: 0.700911\n",
            "Epoch 27: Training Loss: 0.484773 Validation Loss: 0.729862\n",
            "Epoch 28: Training Loss: 0.479517 Validation Loss: 0.705103\n",
            "Epoch 29: Training Loss: 0.474077 Validation Loss: 0.732822\n",
            "Epoch 30: Training Loss: 0.480340 Validation Loss: 0.677832\n",
            "\t  valid_loss decreased (0.690600 --> 0.677832)  saving model...\n",
            "\n",
            "run(lr=0.001, batch_size=256)\n",
            "Epoch  1: Training Loss: 1.499917 Validation Loss: 1.360126\n",
            "\t  valid_loss decreased (inf --> 1.360126)  saving model...\n",
            "Epoch  2: Training Loss: 1.154340 Validation Loss: 1.100079\n",
            "\t  valid_loss decreased (1.360126 --> 1.100079)  saving model...\n",
            "Epoch  3: Training Loss: 0.999789 Validation Loss: 0.937930\n",
            "\t  valid_loss decreased (1.100079 --> 0.937930)  saving model...\n",
            "Epoch  4: Training Loss: 0.919536 Validation Loss: 0.922435\n",
            "\t  valid_loss decreased (0.937930 --> 0.922435)  saving model...\n",
            "Epoch  5: Training Loss: 0.850154 Validation Loss: 0.927689\n",
            "Epoch  6: Training Loss: 0.850527 Validation Loss: 0.875500\n",
            "\t  valid_loss decreased (0.922435 --> 0.875500)  saving model...\n",
            "Epoch  7: Training Loss: 0.796771 Validation Loss: 0.826469\n",
            "\t  valid_loss decreased (0.875500 --> 0.826469)  saving model...\n",
            "Epoch  8: Training Loss: 0.751028 Validation Loss: 0.839540\n",
            "Epoch  9: Training Loss: 0.744918 Validation Loss: 0.815466\n",
            "\t  valid_loss decreased (0.826469 --> 0.815466)  saving model...\n",
            "Epoch 10: Training Loss: 0.703891 Validation Loss: 0.813358\n",
            "\t  valid_loss decreased (0.815466 --> 0.813358)  saving model...\n",
            "Epoch 11: Training Loss: 0.677553 Validation Loss: 0.751369\n",
            "\t  valid_loss decreased (0.813358 --> 0.751369)  saving model...\n",
            "Epoch 12: Training Loss: 0.637959 Validation Loss: 0.750838\n",
            "\t  valid_loss decreased (0.751369 --> 0.750838)  saving model...\n",
            "Epoch 13: Training Loss: 0.606715 Validation Loss: 0.790636\n",
            "Epoch 14: Training Loss: 0.600576 Validation Loss: 0.731435\n",
            "\t  valid_loss decreased (0.750838 --> 0.731435)  saving model...\n",
            "Epoch 15: Training Loss: 0.573214 Validation Loss: 0.759268\n",
            "Epoch 16: Training Loss: 0.576544 Validation Loss: 0.769480\n",
            "Epoch 17: Training Loss: 0.571501 Validation Loss: 0.730739\n",
            "\t  valid_loss decreased (0.731435 --> 0.730739)  saving model...\n",
            "Epoch 18: Training Loss: 0.547127 Validation Loss: 0.727570\n",
            "\t  valid_loss decreased (0.730739 --> 0.727570)  saving model...\n",
            "Epoch 19: Training Loss: 0.520539 Validation Loss: 0.724848\n",
            "\t  valid_loss decreased (0.727570 --> 0.724848)  saving model...\n",
            "Epoch 20: Training Loss: 0.497858 Validation Loss: 0.726742\n",
            "Epoch 21: Training Loss: 0.499672 Validation Loss: 0.711263\n",
            "\t  valid_loss decreased (0.724848 --> 0.711263)  saving model...\n",
            "Epoch 22: Training Loss: 0.472299 Validation Loss: 0.715142\n",
            "Epoch 23: Training Loss: 0.477656 Validation Loss: 0.727287\n",
            "Epoch 24: Training Loss: 0.469009 Validation Loss: 0.731442\n",
            "Epoch 25: Training Loss: 0.472465 Validation Loss: 0.753971\n",
            "Epoch 26: Training Loss: 0.474972 Validation Loss: 0.732922\n",
            "Epoch 27: Training Loss: 0.468265 Validation Loss: 0.708566\n",
            "\t  valid_loss decreased (0.711263 --> 0.708566)  saving model...\n",
            "Epoch 28: Training Loss: 0.457925 Validation Loss: 0.701718\n",
            "\t  valid_loss decreased (0.708566 --> 0.701718)  saving model...\n",
            "Epoch 29: Training Loss: 0.433862 Validation Loss: 0.704850\n",
            "Epoch 30: Training Loss: 0.434186 Validation Loss: 0.726590\n",
            "\n",
            "run(lr=0.001, batch_size=512)\n",
            "Epoch  1: Training Loss: 1.610123 Validation Loss: 1.339367\n",
            "\t  valid_loss decreased (inf --> 1.339367)  saving model...\n",
            "Epoch  2: Training Loss: 1.260011 Validation Loss: 1.155013\n",
            "\t  valid_loss decreased (1.339367 --> 1.155013)  saving model...\n",
            "Epoch  3: Training Loss: 1.092100 Validation Loss: 0.980478\n",
            "\t  valid_loss decreased (1.155013 --> 0.980478)  saving model...\n",
            "Epoch  4: Training Loss: 0.994038 Validation Loss: 0.968991\n",
            "\t  valid_loss decreased (0.980478 --> 0.968991)  saving model...\n",
            "Epoch  5: Training Loss: 0.917176 Validation Loss: 0.946324\n",
            "\t  valid_loss decreased (0.968991 --> 0.946324)  saving model...\n",
            "Epoch  6: Training Loss: 0.862400 Validation Loss: 0.952477\n",
            "Epoch  7: Training Loss: 0.862744 Validation Loss: 0.900045\n",
            "\t  valid_loss decreased (0.946324 --> 0.900045)  saving model...\n",
            "Epoch  8: Training Loss: 0.829392 Validation Loss: 0.908967\n",
            "Epoch  9: Training Loss: 0.811705 Validation Loss: 0.846646\n",
            "\t  valid_loss decreased (0.900045 --> 0.846646)  saving model...\n",
            "Epoch 10: Training Loss: 0.778327 Validation Loss: 0.821719\n",
            "\t  valid_loss decreased (0.846646 --> 0.821719)  saving model...\n",
            "Epoch 11: Training Loss: 0.732320 Validation Loss: 0.841687\n",
            "Epoch 12: Training Loss: 0.742616 Validation Loss: 0.840642\n",
            "Epoch 13: Training Loss: 0.737955 Validation Loss: 0.814668\n",
            "\t  valid_loss decreased (0.821719 --> 0.814668)  saving model...\n",
            "Epoch 14: Training Loss: 0.699616 Validation Loss: 0.844602\n",
            "Epoch 15: Training Loss: 0.716428 Validation Loss: 0.811610\n",
            "\t  valid_loss decreased (0.814668 --> 0.811610)  saving model...\n",
            "Epoch 16: Training Loss: 0.678829 Validation Loss: 0.948917\n",
            "Epoch 17: Training Loss: 0.671497 Validation Loss: 0.849627\n",
            "Epoch 18: Training Loss: 0.662352 Validation Loss: 0.856858\n",
            "Epoch 19: Training Loss: 0.672903 Validation Loss: 0.811318\n",
            "\t  valid_loss decreased (0.811610 --> 0.811318)  saving model...\n",
            "Epoch 20: Training Loss: 0.646597 Validation Loss: 0.784071\n",
            "\t  valid_loss decreased (0.811318 --> 0.784071)  saving model...\n",
            "Epoch 21: Training Loss: 0.623429 Validation Loss: 0.793941\n",
            "Epoch 22: Training Loss: 0.621892 Validation Loss: 0.770714\n",
            "\t  valid_loss decreased (0.784071 --> 0.770714)  saving model...\n",
            "Epoch 23: Training Loss: 0.592385 Validation Loss: 0.804857\n",
            "Epoch 24: Training Loss: 0.597745 Validation Loss: 0.737160\n",
            "\t  valid_loss decreased (0.770714 --> 0.737160)  saving model...\n",
            "Epoch 25: Training Loss: 0.573190 Validation Loss: 0.788303\n",
            "Epoch 26: Training Loss: 0.560788 Validation Loss: 0.762058\n",
            "Epoch 27: Training Loss: 0.561459 Validation Loss: 0.793141\n",
            "Epoch 28: Training Loss: 0.568365 Validation Loss: 0.783223\n",
            "Epoch 29: Training Loss: 0.569012 Validation Loss: 0.784497\n",
            "Epoch 30: Training Loss: 0.575036 Validation Loss: 0.750699\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}