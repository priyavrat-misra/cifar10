{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "0e0d1c7ef0f381ce9c31735005e25185fd13b9c57d8e85878ff9ff982cb55e39"
        }
      }
    },
    "colab": {
      "name": "train_with_vgg16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfIDt-ZZsIHm"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import device, get_num_correct\n",
        "from vgg16modified import Network"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrVtDiKMMQoG"
      },
      "source": [
        "# declare the transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # add augmentations\n",
        "        transforms.ColorJitter(brightness=0.25, saturation=0.1),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "        # We transform them to Tensors of normalized range [-1, 1]\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGmTRbzEsIHr",
        "outputId": "6e3dbe4a-3c93-4141-bff7-518cf71ba441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# choose the training and test datasets\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_transforms['train']\n",
        ")\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transforms['test']\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoWPNbbVsIH0"
      },
      "source": [
        "batch_size = 256\n",
        "valid_size = 0.5  # percentage of test_set to be used as validation\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_test = len(test_set)\n",
        "indices = list(range(num_test))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_test))\n",
        "test_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "# prepare the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "valid_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=test_sampler, num_workers=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQ8mJNfPSeD",
        "outputId": "89ef0199-f07d-486b-af59-ca26502e3852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "vgg16"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umFYw1-cW_jM",
        "outputId": "b7437732-2024-4ba6-a6b2-9e7e90abc5db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# replace the vgg16 classifier\n",
        "model = Network(vgg16)\n",
        "model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pDgSH74Xlhp"
      },
      "source": [
        "# transfer learning (first 5 layers of vgg16)\n",
        "# freeze the transferred weights which won't be trained\n",
        "for layer_num, child in enumerate(model.features.children()):\n",
        "    if layer_num < 19:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUq5gbZRsIH6",
        "outputId": "279444a8-f7f3-4539-8a3e-324335ee044f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # loss function (categorical cross-entropy)\n",
        "optimizer = optim.SGD(\n",
        "    [      # parameters which need optimization\n",
        "        {'params':model.features[19:].parameters(), 'lr':0.03},\n",
        "        {'params':model.classifier.parameters()}\n",
        "    ], lr=0.1, momentum=0.9, weight_decay=1e-3)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/3, patience=5, verbose=True) # lr scheduler\n",
        "\n",
        "comment = f'-transferlr_vgg16'  # will be used for naming the run\n",
        "tb = SummaryWriter(comment=comment)\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial minimum to infinity\n",
        "num_epochs = 30  # number of epochs used for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_correct = 0, 0  # wil be used to track the running loss and correct\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    train_loop = tqdm(train_loader, total=len(train_loader))\n",
        "    model.train()  # set the model to train mode\n",
        "    for batch in train_loop:\n",
        "        images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device (cpu/gpu)\n",
        "        preds = model(images)  # forward pass\n",
        "        loss = criterion(preds, labels)  # calculate loss\n",
        "        optimizer.zero_grad()  # clear accumulated gradients from the previous pass\n",
        "        loss.backward()  # backward pass\n",
        "        optimizer.step()  # perform a single optimization step\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0) # update the running loss\n",
        "        train_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        train_loop.set_description(f'Epoch [{epoch+1:2d}/{num_epochs}]')\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=train_correct/len(train_set))\n",
        "\n",
        "    # add train loss and train accuracy for the current epoch to tensorboard\n",
        "    tb.add_scalar('Train Loss', train_loss, epoch)\n",
        "    tb.add_scalar('Train Accuracy', train_correct/len(train_set), epoch)\n",
        "\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    with torch.no_grad():  # turn off grad tracking, as we don't need gradients for validation\n",
        "        valid_loss, valid_correct = 0, 0  # will be used to track the running validation loss and correct\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        for batch in valid_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            valid_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            valid_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "            \n",
        "\n",
        "        # add validation loss and validation accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Validation Loss', valid_loss, epoch)\n",
        "        tb.add_scalar('Validation Accuracy', valid_correct/len(valid_loader.sampler), epoch)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(train_set)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        train_loop.write(f'\\t\\tAvg training loss: {train_loss:.6f}\\tAvg validation loss: {valid_loss:.6f}')\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            train_loop.write(f'\\t\\tvalid_loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f})  saving model...')\n",
        "            torch.save(model.state_dict(), f'./models/model{comment}.pth')\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "\n",
        "        test_loss, test_correct = 0, 0  # will be used to track the running test loss and correct\n",
        "        ##################\n",
        "        # test the model #\n",
        "        ##################\n",
        "        for batch in test_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            test_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            test_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        # add test loss and test accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Test Loss', test_loss, epoch)\n",
        "        tb.add_scalar('Test Accuracy', test_correct/len(test_loader.sampler), epoch)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 1/30]: 100%|██████████| 196/196 [00:42<00:00,  4.56it/s, acc=0.639, loss=0.904]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 1.060830\tAvg validation loss: 0.725085\n",
            "\t\tvalid_loss decreased (inf --> 0.725085)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 2/30]: 100%|██████████| 196/196 [00:42<00:00,  4.61it/s, acc=0.767, loss=0.804]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.687660\tAvg validation loss: 0.712803\n",
            "\t\tvalid_loss decreased (0.725085 --> 0.712803)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 3/30]: 100%|██████████| 196/196 [00:42<00:00,  4.64it/s, acc=0.802, loss=0.633]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.581167\tAvg validation loss: 0.486661\n",
            "\t\tvalid_loss decreased (0.712803 --> 0.486661)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 4/30]: 100%|██████████| 196/196 [00:41<00:00,  4.71it/s, acc=0.82, loss=0.595]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.528874\tAvg validation loss: 0.513192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 5/30]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.836, loss=0.466]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.485242\tAvg validation loss: 0.468349\n",
            "\t\tvalid_loss decreased (0.486661 --> 0.468349)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 6/30]: 100%|██████████| 196/196 [00:41<00:00,  4.77it/s, acc=0.847, loss=0.422]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.447901\tAvg validation loss: 0.532781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 7/30]: 100%|██████████| 196/196 [00:41<00:00,  4.78it/s, acc=0.851, loss=0.498]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.435389\tAvg validation loss: 0.461138\n",
            "\t\tvalid_loss decreased (0.468349 --> 0.461138)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 8/30]: 100%|██████████| 196/196 [00:41<00:00,  4.77it/s, acc=0.862, loss=0.287]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.408889\tAvg validation loss: 0.445688\n",
            "\t\tvalid_loss decreased (0.461138 --> 0.445688)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 9/30]: 100%|██████████| 196/196 [00:40<00:00,  4.80it/s, acc=0.866, loss=0.488]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.390850\tAvg validation loss: 0.432318\n",
            "\t\tvalid_loss decreased (0.445688 --> 0.432318)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/30]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.874, loss=0.63]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.371673\tAvg validation loss: 0.436332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11/30]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.878, loss=0.281]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.356769\tAvg validation loss: 0.432596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12/30]: 100%|██████████| 196/196 [00:41<00:00,  4.73it/s, acc=0.882, loss=0.247]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.343391\tAvg validation loss: 0.432879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13/30]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.885, loss=0.264]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.332500\tAvg validation loss: 0.435752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14/30]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.891, loss=0.275]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.322740\tAvg validation loss: 0.433673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15/30]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.895, loss=0.301]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.309547\tAvg validation loss: 0.494816\n",
            "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
            "Epoch    15: reducing learning rate of group 1 to 3.3333e-02.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16/30]: 100%|██████████| 196/196 [00:41<00:00,  4.72it/s, acc=0.932, loss=0.151]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.197079\tAvg validation loss: 0.358085\n",
            "\t\tvalid_loss decreased (0.432318 --> 0.358085)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17/30]: 100%|██████████| 196/196 [00:41<00:00,  4.78it/s, acc=0.944, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.161911\tAvg validation loss: 0.386529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18/30]: 100%|██████████| 196/196 [00:40<00:00,  4.84it/s, acc=0.95, loss=0.24]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.143179\tAvg validation loss: 0.407683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19/30]: 100%|██████████| 196/196 [00:40<00:00,  4.84it/s, acc=0.953, loss=0.123]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.137749\tAvg validation loss: 0.397495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20/30]: 100%|██████████| 196/196 [00:39<00:00,  4.92it/s, acc=0.956, loss=0.0684]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.129278\tAvg validation loss: 0.417324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21/30]: 100%|██████████| 196/196 [00:40<00:00,  4.88it/s, acc=0.958, loss=0.11]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.122153\tAvg validation loss: 0.428293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22/30]: 100%|██████████| 196/196 [00:39<00:00,  4.93it/s, acc=0.962, loss=0.0788]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.114404\tAvg validation loss: 0.401970\n",
            "Epoch    22: reducing learning rate of group 0 to 3.3333e-03.\n",
            "Epoch    22: reducing learning rate of group 1 to 1.1111e-02.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23/30]: 100%|██████████| 196/196 [00:40<00:00,  4.87it/s, acc=0.973, loss=0.0356]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.078829\tAvg validation loss: 0.394227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24/30]: 100%|██████████| 196/196 [00:39<00:00,  4.91it/s, acc=0.977, loss=0.0903]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.067433\tAvg validation loss: 0.414836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25/30]: 100%|██████████| 196/196 [00:40<00:00,  4.88it/s, acc=0.979, loss=0.0569]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.059944\tAvg validation loss: 0.407773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26/30]: 100%|██████████| 196/196 [00:39<00:00,  4.97it/s, acc=0.982, loss=0.0328]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.053937\tAvg validation loss: 0.416741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27/30]: 100%|██████████| 196/196 [00:39<00:00,  4.92it/s, acc=0.983, loss=0.0129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.049537\tAvg validation loss: 0.423936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28/30]: 100%|██████████| 196/196 [00:39<00:00,  4.96it/s, acc=0.984, loss=0.0377]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.048497\tAvg validation loss: 0.440331\n",
            "Epoch    28: reducing learning rate of group 0 to 1.1111e-03.\n",
            "Epoch    28: reducing learning rate of group 1 to 3.7037e-03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29/30]: 100%|██████████| 196/196 [00:39<00:00,  4.92it/s, acc=0.987, loss=0.0769]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.039893\tAvg validation loss: 0.435346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30/30]: 100%|██████████| 196/196 [00:39<00:00,  4.93it/s, acc=0.988, loss=0.0275]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.038943\tAvg validation loss: 0.436046\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}