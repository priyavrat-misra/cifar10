{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train_with_vgg16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfIDt-ZZsIHm"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import device, get_num_correct"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrVtDiKMMQoG"
      },
      "source": [
        "# declare the transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # add augmentations\n",
        "        transforms.ColorJitter(brightness=0.25, saturation=0.1),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "        # We transform them to Tensors of normalized range [-1, 1]\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGmTRbzEsIHr",
        "outputId": "21005a63-978c-4672-92ce-68ea84970ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# choose the training and test datasets\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_transforms['train']\n",
        ")\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data/',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transforms['test']\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoWPNbbVsIH0"
      },
      "source": [
        "batch_size = 256\n",
        "valid_size = 0.5  # percentage of test_set to be used as validation\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_test = len(test_set)\n",
        "indices = list(range(num_test))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_test))\n",
        "test_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "# prepare the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "valid_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=test_sampler, num_workers=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQ8mJNfPSeD",
        "outputId": "d2c75755-5502-431e-f2c0-103d046d59c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "vgg16"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBpc5TOHWD2d"
      },
      "source": [
        "# modify the network as per requirement\n",
        "class vgg16modified(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(*list(vgg16.features.children())[:19])\n",
        "        self.need_train = nn.Sequential(*list(vgg16.features.children())[19:])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=128, out_features=10)\n",
        "            )\n",
        "        \n",
        "    def forward(self, t):\n",
        "        t = self.features(t)\n",
        "        t = self.need_train(t)\n",
        "        t = torch.flatten(t, 1)\n",
        "        t = self.classifier(t)\n",
        "\n",
        "        return t"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umFYw1-cW_jM",
        "outputId": "97b00d91-c5f3-471f-94f1-6cf2830a5a69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = vgg16modified()\n",
        "model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "vgg16modified(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "  )\n",
              "  (need_train): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeAANJJJPSeJ"
      },
      "source": [
        "# freeze the parameters which don't need training\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUq5gbZRsIH6",
        "outputId": "af46910e-acff-449c-fa5e-810a77fb7483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.to(device)\n",
        "\n",
        "lr = 0.1\n",
        "criterion = nn.CrossEntropyLoss()  # loss function (categorical cross-entropy)\n",
        "params = list(model.need_train.parameters()) + list(model.classifier.parameters())\n",
        "optimizer = optim.SGD(params, lr=lr)  # specify the optimizer\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/3, patience=5, verbose=True) # lr scheduler\n",
        "\n",
        "comment = f'-transferlr_vgg16(bsize={batch_size})'  # will be used for naming the run\n",
        "tb = SummaryWriter(comment=comment)\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial minimum to infinity\n",
        "num_epochs = 50  # number of epochs used for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_correct = 0, 0  # wil be used to track the running loss and correct\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    train_loop = tqdm(train_loader, total=len(train_loader))\n",
        "    model.train()  # set the model to train mode\n",
        "    for batch in train_loop:\n",
        "        images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device (cpu/gpu)\n",
        "        preds = model(images)  # forward pass\n",
        "        loss = criterion(preds, labels)  # calculate loss\n",
        "        optimizer.zero_grad()  # clear accumulated gradients from the previous pass\n",
        "        loss.backward()  # backward pass\n",
        "        optimizer.step()  # perform a single optimization step\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0) # update the running loss\n",
        "        train_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        train_loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=train_correct/len(train_set))\n",
        "\n",
        "    # add train loss and train accuracy for the current epoch to tensorboard\n",
        "    tb.add_scalar('Train Loss', train_loss, epoch)\n",
        "    tb.add_scalar('Train Accuracy', train_correct/len(train_set), epoch)\n",
        "\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    with torch.no_grad():  # turn off grad tracking, as we don't need gradients for validation\n",
        "        valid_loss, valid_correct = 0, 0  # will be used to track the running validation loss and correct\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        for batch in valid_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            valid_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            valid_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "            \n",
        "\n",
        "        # add validation loss and validation accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Validation Loss', valid_loss, epoch)\n",
        "        tb.add_scalar('Validation Accuracy', valid_correct/len(valid_loader.sampler), epoch)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(train_set)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        train_loop.write(f'\\t\\tAvg training loss: {train_loss:.6f}\\tAvg validation loss: {valid_loss:.6f}')\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            train_loop.write(f'\\t\\tvalid_loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f})  saving model...')\n",
        "            torch.save(model.state_dict(), f'./models/model{comment}.pth')\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "\n",
        "        test_loss, test_correct = 0, 0  # will be used to track the running test loss and correct\n",
        "        ##################\n",
        "        # test the model #\n",
        "        ##################\n",
        "        for batch in test_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            test_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            test_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        # add test loss and test accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Test Loss', test_loss, epoch)\n",
        "        tb.add_scalar('Test Accuracy', test_correct/len(test_loader.sampler), epoch)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50]: 100%|██████████| 196/196 [00:41<00:00,  4.69it/s, acc=0.425, loss=1.05]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 1.609961\tAvg validation loss: 1.191050\n",
            "\t\tvalid_loss decreased (inf --> 1.191050)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/50]: 100%|██████████| 196/196 [00:41<00:00,  4.73it/s, acc=0.696, loss=0.707]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.895720\tAvg validation loss: 0.662040\n",
            "\t\tvalid_loss decreased (1.191050 --> 0.662040)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.764, loss=0.664]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.691515\tAvg validation loss: 1.184928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4/50]: 100%|██████████| 196/196 [00:40<00:00,  4.79it/s, acc=0.788, loss=0.607]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.614401\tAvg validation loss: 0.548118\n",
            "\t\tvalid_loss decreased (0.662040 --> 0.548118)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.807, loss=0.426]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.552967\tAvg validation loss: 0.568714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.824, loss=0.511]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.507663\tAvg validation loss: 0.516603\n",
            "\t\tvalid_loss decreased (0.548118 --> 0.516603)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.837, loss=0.614]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.467517\tAvg validation loss: 0.601821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.849, loss=0.392]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.430958\tAvg validation loss: 0.524161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.859, loss=0.259]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.402474\tAvg validation loss: 0.454039\n",
            "\t\tvalid_loss decreased (0.516603 --> 0.454039)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.867, loss=0.347]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.379196\tAvg validation loss: 0.492217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.875, loss=0.333]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.354135\tAvg validation loss: 0.479978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.883, loss=0.39]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.330846\tAvg validation loss: 0.577499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.892, loss=0.239]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.310191\tAvg validation loss: 0.477957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.899, loss=0.376]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.286275\tAvg validation loss: 0.728352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15/50]: 100%|██████████| 196/196 [00:41<00:00,  4.73it/s, acc=0.906, loss=0.191]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.269341\tAvg validation loss: 0.488498\n",
            "Epoch    15: reducing learning rate of group 0 to 3.3333e-02.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16/50]: 100%|██████████| 196/196 [00:41<00:00,  4.72it/s, acc=0.934, loss=0.175]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.187682\tAvg validation loss: 0.459692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.94, loss=0.143]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.171454\tAvg validation loss: 0.469020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.942, loss=0.166]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.164494\tAvg validation loss: 0.451573\n",
            "\t\tvalid_loss decreased (0.454039 --> 0.451573)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19/50]: 100%|██████████| 196/196 [00:41<00:00,  4.77it/s, acc=0.947, loss=0.148]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.151451\tAvg validation loss: 0.477853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.95, loss=0.158]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.141050\tAvg validation loss: 0.501092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21/50]: 100%|██████████| 196/196 [00:40<00:00,  4.79it/s, acc=0.952, loss=0.215]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.136179\tAvg validation loss: 0.493665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.954, loss=0.0924]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.128966\tAvg validation loss: 0.496725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23/50]: 100%|██████████| 196/196 [00:40<00:00,  4.79it/s, acc=0.957, loss=0.159]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.123065\tAvg validation loss: 0.564564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24/50]: 100%|██████████| 196/196 [00:41<00:00,  4.77it/s, acc=0.958, loss=0.04]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.117405\tAvg validation loss: 0.504744\n",
            "Epoch    24: reducing learning rate of group 0 to 1.1111e-02.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.967, loss=0.057]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.097268\tAvg validation loss: 0.512511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.969, loss=0.0503]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.088173\tAvg validation loss: 0.528223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27/50]: 100%|██████████| 196/196 [00:40<00:00,  4.78it/s, acc=0.97, loss=0.0578]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.084758\tAvg validation loss: 0.531362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.97, loss=0.0941]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.086011\tAvg validation loss: 0.533336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29/50]: 100%|██████████| 196/196 [00:41<00:00,  4.69it/s, acc=0.971, loss=0.0555]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.082620\tAvg validation loss: 0.539577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30/50]: 100%|██████████| 196/196 [00:41<00:00,  4.73it/s, acc=0.972, loss=0.0419]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.080705\tAvg validation loss: 0.548233\n",
            "Epoch    30: reducing learning rate of group 0 to 3.7037e-03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [31/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.973, loss=0.059]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.076825\tAvg validation loss: 0.553137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [32/50]: 100%|██████████| 196/196 [00:41<00:00,  4.73it/s, acc=0.975, loss=0.0886]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.071920\tAvg validation loss: 0.553377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [33/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.975, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.070319\tAvg validation loss: 0.555232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [34/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.975, loss=0.0405]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.072448\tAvg validation loss: 0.558689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [35/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.976, loss=0.0888]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.068376\tAvg validation loss: 0.564666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [36/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.977, loss=0.0172]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.069103\tAvg validation loss: 0.564100\n",
            "Epoch    36: reducing learning rate of group 0 to 1.2346e-03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [37/50]: 100%|██████████| 196/196 [00:40<00:00,  4.80it/s, acc=0.977, loss=0.106]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.066935\tAvg validation loss: 0.564717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [38/50]: 100%|██████████| 196/196 [00:40<00:00,  4.81it/s, acc=0.977, loss=0.0668]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.065386\tAvg validation loss: 0.565944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [39/50]: 100%|██████████| 196/196 [00:41<00:00,  4.75it/s, acc=0.978, loss=0.0139]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.067508\tAvg validation loss: 0.566170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [40/50]: 100%|██████████| 196/196 [00:41<00:00,  4.77it/s, acc=0.977, loss=0.0643]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.065646\tAvg validation loss: 0.570024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [41/50]: 100%|██████████| 196/196 [00:42<00:00,  4.66it/s, acc=0.978, loss=0.0587]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.065055\tAvg validation loss: 0.570301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [42/50]: 100%|██████████| 196/196 [00:40<00:00,  4.80it/s, acc=0.978, loss=0.0461]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.064315\tAvg validation loss: 0.573000\n",
            "Epoch    42: reducing learning rate of group 0 to 4.1152e-04.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [43/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.977, loss=0.032]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.064879\tAvg validation loss: 0.573100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [44/50]: 100%|██████████| 196/196 [00:41<00:00,  4.76it/s, acc=0.978, loss=0.0368]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.063979\tAvg validation loss: 0.572865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [45/50]: 100%|██████████| 196/196 [00:41<00:00,  4.74it/s, acc=0.979, loss=0.0785]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.063747\tAvg validation loss: 0.573190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [46/50]: 100%|██████████| 196/196 [00:40<00:00,  4.78it/s, acc=0.978, loss=0.0301]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.065863\tAvg validation loss: 0.574302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [47/50]: 100%|██████████| 196/196 [00:42<00:00,  4.57it/s, acc=0.979, loss=0.0343]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.062233\tAvg validation loss: 0.575469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [48/50]: 100%|██████████| 196/196 [00:43<00:00,  4.56it/s, acc=0.978, loss=0.0723]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.064981\tAvg validation loss: 0.575820\n",
            "Epoch    48: reducing learning rate of group 0 to 1.3717e-04.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [49/50]: 100%|██████████| 196/196 [00:42<00:00,  4.59it/s, acc=0.978, loss=0.0271]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.064553\tAvg validation loss: 0.575631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [50/50]: 100%|██████████| 196/196 [00:42<00:00,  4.58it/s, acc=0.979, loss=0.0458]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.063950\tAvg validation loss: 0.575285\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}